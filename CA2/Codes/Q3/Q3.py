# -*- coding: utf-8 -*-
"""Part3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lB6oNl-ZScHqkzKJdfkUZXSN-TGwVVmw
"""

import tensorflow as tf
from keras import layers, models,Model
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import cifar10
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import SGD
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
import time
import seaborn as sns
from keras.layers import Input

"""# PART A"""

(train_data, train_label), (test_data, test_label) = cifar10.load_data()
train_data, valid_data, train_label, valid_label = train_test_split(train_data, train_label, test_size=0.2, random_state=42)

grey_train = tf.image.rgb_to_grayscale(train_data)
grey_valid = tf.image.rgb_to_grayscale(valid_data)
grey_test = tf.image.rgb_to_grayscale(test_data)

grey_train=np.squeeze(np.array(grey_train)).reshape(40000,32*32)
grey_valid=np.squeeze(np.array(grey_valid)).reshape(10000,32*32)
grey_test=np.squeeze(np.array(grey_test)).reshape(10000,32*32)

train_data = np.array(grey_train) / 255.0
valid_data= np.array(grey_valid)/ 255.0
test_data = np.array(grey_test) / 255.0

print(train_data.shape)
print(valid_data.shape)
print(test_data.shape)

train_label = tf.keras.utils.to_categorical(train_label,10)
test_label = tf.keras.utils.to_categorical(test_label,10)
valid_label = tf.keras.utils.to_categorical(valid_label,10)

from sklearn.utils.extmath import randomized_svd
U, S, Vt = randomized_svd(
                train_data,
                n_components=1024,
                n_iter="auto",
                random_state=None,
            )

eigs=S**2/(40000-1)

var_exp = [(i / sum(eigs)) for i in sorted(eigs, reverse=True)]
print(var_exp)
cum_var_exp = np.cumsum(var_exp)

plt.plot(range(1,100), cum_var_exp[0:99], 
         label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

# using first 77 components to transform data
train_img_pca = train_data@Vt[0:77,:].T
test_img_pca = test_data@Vt[0:77,:].T
val_img_pca = valid_data@Vt[0:77,:].T

print(train_img_pca.shape)
print(test_img_pca.shape)
print(val_img_pca.shape)

model = models.Sequential()
model.add(layers.Dense(1024, activation='relu', input_shape=(77,)))
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.summary()

# training the model
start_time=time.time()
opt = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
fit_history = model.fit(train_img_pca, train_label, epochs=20,
                    validation_data=(val_img_pca, valid_label), batch_size=128)
stop_time=time.time()
print("\n training time:",stop_time-start_time)

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['accuracy'], label='training accuracy')
plt.plot(fit_history.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
#plt.ylim([0.4, 1])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['loss'], label='training loss')
plt.plot(fit_history.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 2])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
z=model.predict(test_img_pca)
pred=np.argmax(z,axis=1)
y_label=np.argmax(test_label,axis=1)
cm = confusion_matrix(pred, y_label)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#disp.plot(cmap=plt.cm.Blues)
sns.heatmap(cm,cmap="Blues",annot=True,fmt="d")
plt.show()

# calculate loss and accuracy on test data and print it
test_loss, test_acc = model.evaluate(test_img_pca, test_label, verbose=2)
train_loss, train_acc = model.evaluate(train_img_pca,  train_label, verbose=2)
precision = precision_score(y_label,pred,average='weighted')
recall = recall_score(y_label,pred,average='weighted')
f1 = f1_score(y_label,pred,average='weighted')
print("accuracy on test data:",test_acc)
print("loss on test data:",test_loss)
print("f1_score on test data:",f1)
print("recall on test data:",recall)
print("presicion on test data:",precision)
#print("accuracy on train data:",train_acc)
#print("loss on train data:",train_loss)

"""# PART B"""

input_img = Input(shape=(1024,))
encoded1 = layers.Dense(1024, activation='relu')(input_img)
encoded2 = layers.Dense(77, activation='relu')(encoded1)
decoded1 = layers.Dense(512, activation='relu')(encoded2)
decoded2 = layers.Dense(1024, activation=None)(decoded1)
autoencoder = Model(input_img, decoded2)
opt = SGD(learning_rate=0.02, momentum=0.9)
autoencoder.compile(optimizer=opt, loss='mse')
autoencoder.summary()

autoencoder.fit(train_data,train_data,
                epochs=70,
                batch_size=64,
                shuffle=False)
# Encoder
encoder = Model(input_img, encoded2)
encoded_imgs = encoder.predict(train_data)

encoded_imgs = encoder.predict(train_data)
encoded_test = encoder.predict(test_data)
encoded_valid = encoder.predict(valid_data)

model = models.Sequential()
model.add(layers.Dense(1024, activation='relu', input_shape=(77,)))
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.summary()

# training the model
start_time=time.time()
opt = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
fit_history = model.fit(encoded_imgs, train_label, epochs=40,
                    validation_data=(encoded_valid, valid_label), batch_size=128)
stop_time=time.time()
print("\n training time:",stop_time-start_time)

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['accuracy'], label='training accuracy')
plt.plot(fit_history.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
#plt.ylim([0.4, 1])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['loss'], label='training loss')
plt.plot(fit_history.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 2])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
z=model.predict(encoded_test)
pred=np.argmax(z,axis=1)
y_label=np.argmax(test_label,axis=1)
cm = confusion_matrix(pred, y_label)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#disp.plot(cmap=plt.cm.Blues)
sns.heatmap(cm,cmap="Blues",annot=True,fmt="d")
plt.show()

# calculate loss and accuracy on test data and print it
test_loss, test_acc = model.evaluate(encoded_test, test_label, verbose=2)
train_loss, train_acc = model.evaluate(encoded_imgs,  train_label, verbose=2)
precision = precision_score(y_label,pred,average='weighted')
recall = recall_score(y_label,pred,average='weighted')
f1 = f1_score(y_label,pred,average='weighted')
print("accuracy on test data:",test_acc)
print("loss on test data:",test_loss)
print("f1_score on test data:",f1)
print("recall on test data:",recall)
print("presicion on test data:",precision)
#print("accuracy on train data:",train_acc)
#print("loss on train data:",train_loss)

"""# PART C"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import keras
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
drive.mount('/content/drive')
# %cd /content/drive/My Drive/HW2_deep/

df = pd.read_csv("data.csv")
df=df[df["price"]<=10000000]

df['date'] = pd.to_datetime(df['date'])
df['month'] = df['date'].apply(lambda date:date.month)
df['year'] = df['date'].apply(lambda date:date.year)
df['day']=df['date'].apply(lambda date:date.day)
y = df.price
X = df.drop(columns=["price"], axis=1)

df=df.drop("date",axis=1)

cat_cols=["street", "city","statezip","country"]
num_cols=list(df.columns.values)
num_cols=list(set(num_cols)-set(cat_cols))
X_categorical_df = pd.get_dummies(X[cat_cols], columns=cat_cols)
#num_cols.remove("price")

cm=df.corr()

plt.figure(figsize=(15, 10), dpi=100)
sns.set(font_scale=1)
hm = sns.heatmap(cm,
                cbar=True,
                annot=True,
                square=True,
                fmt='.2f',
                 )

"""# PART D"""

num_cols.remove("price")
X_final = X[num_cols]
X_final = X_final.join(X_categorical_df)

from sklearn import preprocessing
X_final[num_cols] = preprocessing.StandardScaler().fit_transform(X_final[num_cols])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=420)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=420)

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# fit the model
model.fit(X_train, y_train)
# get importance
importance = model.coef_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
#plt.bar([x for x in range(len(importance))], importance)
#plt.show()

from copy import deepcopy
# sort features by importance
bb=deepcopy(importance)
bb=abs(bb)
bb.sort()
bb=np.flip(bb)
zz=[np.where(abs(importance)==bb[i]) for i in range(14) ]
zxc=[x[0][0] for x in zz]
feats=[X_train.iloc[:,i].name for i in zxc]

plt.figure(figsize=(14,9))
plt.bar(feats[0:14],bb[0:14])
plt.ylabel("importance")
plt.xlabel("feature")
plt.xticks(rotation='vertical')
plt.tight_layout

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
# fit the model
model.fit(X_train, y_train)
# get importance
importance = model.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))

from copy import deepcopy
# sort features by importance
bb=deepcopy(importance)
bb=abs(bb)
bb.sort()
bb=np.flip(bb)
zz=[np.where(abs(importance)==bb[i]) for i in range(14) ]
zxc=[x[0][0] for x in zz]
feats=[X_train.iloc[:,i].name for i in zxc]

plt.figure(figsize=(14,9))
plt.bar(feats,bb[0:14])
plt.ylabel("importance")
plt.xlabel("feature")
plt.xticks(rotation='vertical')
plt.tight_layout

from sklearn.utils.extmath import randomized_svd
U, S, Vt = randomized_svd(
                X_train.values,
                n_components=100,
                n_iter="auto",
                random_state=None,
            )

eigs=S**2/(3126-1)

var_exp = [(i / sum(eigs)) for i in sorted(eigs, reverse=True)]
print(var_exp)
cum_var_exp = np.cumsum(var_exp)

plt.plot(range(14), cum_var_exp[0:14], 
         label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

X_train = X_train@Vt.T
X_test = X_test@Vt.T
X_val = X_val@Vt.T

X_train.shape, X_test.shape, y_train.shape, y_test.shape,y_val.shape, X_val.shape

model = models.Sequential()
model.add(layers.Dense(60,activation='relu'))
model.add(layers.Dense(60,activation='relu'))
model.add(layers.Dense(60,activation='relu'))
#model.add(layers.Dense(20,activation='relu'))
model.add(layers.Dense(1))
model.compile(optimizer='Adam',loss='mae')
model.fit(x=X_train,y=y_train,
          validation_data=(X_val,y_val),
          batch_size=128,epochs=200)
model.summary()
loss_df = pd.DataFrame(model.history.history)

loss_df.plot(figsize=(12,8))
plt.xlabel("Epoch")
plt.ylabel("Loss")
y_pred = model.predict(X_test)
from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))  
print('MSE:', metrics.mean_squared_error(y_test, y_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,y_pred))# Visualizing Our predictions
fig = plt.figure(figsize=(10,5))
plt.scatter(y_test,y_pred)
# Perfect predictions
plt.plot(y_test,y_test,'r')
plt.xlabel("num_house")
plt.ylabel("predicted price")