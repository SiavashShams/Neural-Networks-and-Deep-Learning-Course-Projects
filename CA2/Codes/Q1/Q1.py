# -*- coding: utf-8 -*-
"""Copy of cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CGVC86TsvvNscMm0j9eRwM0l_a8AcTvD

# **PART A: Number of Neurons**
"""

import tensorflow as tf
from keras import layers, models
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import cifar10
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import SGD
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
import time
import seaborn as sns

# load data
(train_data, train_label), (test_data, test_label) = cifar10.load_data()
samples = [29, 32, 24, 21, 28, 27, 25, 37, 62, 67]
# plot image of each class
plt.figure(0)
for i in range(5):
    for j in range(2):
        plt.subplot2grid((5, 2), (i, j))
        plt.imshow(train_data[samples[2 * i + j]])
plt.show()
train_data, valid_data, train_label, valid_label = train_test_split(train_data, train_label, test_size=0.2, random_state=42)

# RGB to Grayscale
grey_train = tf.image.rgb_to_grayscale(train_data)
grey_valid = tf.image.rgb_to_grayscale(valid_data)
grey_test = tf.image.rgb_to_grayscale(test_data)

# Reshaping to 1*1024
grey_train=np.squeeze(np.array(grey_train)).reshape(40000,32*32)
grey_valid=np.squeeze(np.array(grey_valid)).reshape(10000,32*32)
grey_test=np.squeeze(np.array(grey_test)).reshape(10000,32*32)

# normalize data
train_data = np.array(grey_train) / 255.0
valid_data= np.array(grey_valid)/ 255.0
test_data = np.array(grey_test) / 255.0

print(train_data.shape)
print(valid_data.shape)
print(test_data.shape)

# one hot encoding
train_label = tf.keras.utils.to_categorical(train_label,10)
test_label = tf.keras.utils.to_categorical(test_label,10)
valid_label = tf.keras.utils.to_categorical(valid_label,10)

# define our model
model = models.Sequential()
model.add(layers.Dense(1024, activation='relu', input_shape=(1024,)))
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.summary()

# training the model
start_time=time.time()
opt = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
fit_history = model.fit(train_data, train_label, epochs=20,
                    validation_data=(valid_data, valid_label), batch_size=64)
stop_time=time.time()
print("\n training time:",stop_time-start_time)

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['accuracy'], label='training accuracy')
plt.plot(fit_history.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['loss'], label='training loss')
plt.plot(fit_history.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 5), dpi=100)
z=model.predict(test_data)
pred=np.argmax(z,axis=1)
y_label=np.argmax(test_label,axis=1)
cm = confusion_matrix(pred, y_label)
sns.heatmap(cm,cmap="Blues",annot=True,fmt="d")
plt.show()

# calculate loss and accuracy on test data and print it
test_loss, test_acc = model.evaluate(test_data, test_label, verbose=2)
train_loss, train_acc = model.evaluate(train_data,  train_label, verbose=2)
precision = precision_score(y_label,pred,average='weighted')
recall = recall_score(y_label,pred,average='weighted')
f1 = f1_score(y_label,pred,average='weighted')
print("accuracy on test data:",test_acc)
print("loss on test data:",test_loss)
print("f1_score on test data:",f1)
print("recall on test data:",recall)
print("presicion on test data:",precision)
#print("accuracy on train data:",train_acc)
#print("loss on train data:",train_loss)

"""# **PART B: Batch Size**"""

model = models.Sequential()
model.add(layers.Dense(1024, activation='relu', input_shape=(1024,)))
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.summary()

# training the model
# change batch size
batch = 512
start_time=time.time()
opt = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
fit_history = model.fit(train_data, train_label, epochs=20,
                    validation_data=(valid_data, valid_label), batch_size=batch)
stop_time=time.time()
print("\n training time:",stop_time-start_time)

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['accuracy'], label='training accuracy')
plt.plot(fit_history.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
#plt.ylim([0.4, 1])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['loss'], label='training loss')
plt.plot(fit_history.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 2])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
z=model.predict(test_data)
pred=np.argmax(z,axis=1)
y_label=np.argmax(test_label,axis=1)
cm = confusion_matrix(pred, y_label)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#disp.plot(cmap=plt.cm.Blues)
sns.heatmap(cm,cmap="Blues",annot=True,fmt="d")
plt.show()

# calculate loss and accuracy on test data and print it
test_loss, test_acc = model.evaluate(test_data, test_label, verbose=2)
train_loss, train_acc = model.evaluate(train_data,  train_label, verbose=2)
precision = precision_score(y_label,pred,average='weighted')
recall = recall_score(y_label,pred,average='weighted')
f1 = f1_score(y_label,pred,average='weighted')
print("accuracy on test data:",test_acc)
print("loss on test data:",test_loss)
print("f1_score on test data:",f1)
print("recall on test data:",recall)
print("presicion on test data:",precision)
#print("accuracy on train data:",train_acc)
#print("loss on train data:",train_loss)

"""# **PART C: Activation Functions**"""

model = models.Sequential()
# change activation function to see different parts
activation = "relu"
model.add(layers.Dense(1024, activation = activation, input_shape=(1024,)))
model.add(layers.Dense(1024, activation = activation))
model.add(layers.Dense(512, activation = activation))
model.add(layers.Dense(10, activation = "softmax"))
model.summary()

# training the model
batch_size = 128
start_time=time.time()
opt = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
fit_history = model.fit(train_data, train_label, epochs=20,
                    validation_data=(valid_data, valid_label), batch_size=batch_size)
stop_time=time.time()
print("\n training time:",stop_time-start_time)

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['accuracy'], label='training accuracy')
plt.plot(fit_history.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
#plt.ylim([0.4, 1])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['loss'], label='training loss')
plt.plot(fit_history.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 2])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
z=model.predict(test_data)
pred=np.argmax(z,axis=1)
y_label=np.argmax(test_label,axis=1)
cm = confusion_matrix(pred, y_label)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#disp.plot(cmap=plt.cm.Blues)
sns.heatmap(cm,cmap="Blues",annot=True,fmt="d")
plt.show()

# calculate loss and accuracy on test data and print it
test_loss, test_acc = model.evaluate(test_data, test_label, verbose=2)
train_loss, train_acc = model.evaluate(train_data,  train_label, verbose=2)
precision = precision_score(y_label,pred,average='weighted')
recall = recall_score(y_label,pred,average='weighted')
f1 = f1_score(y_label,pred,average='weighted')
print("accuracy on test data:",test_acc)
print("loss on test data:",test_loss)
print("f1_score on test data:",f1)
print("recall on test data:",recall)
print("presicion on test data:",precision)

"""# **PART D: Loss Function**"""

model = models.Sequential()
activation = "relu"
model.add(layers.Dense(1024, activation = "relu", input_shape=(1024,)))
model.add(layers.Dense(1024, activation = "relu"))
model.add(layers.Dense(512, activation = "relu"))
model.add(layers.Dense(10, activation = "softmax"))
model.summary()

# training the model
batch_size = 128
start_time=time.time()
opt = SGD(learning_rate=0.01, momentum=0.9)
loss_func = "categorical_crossentropy"
# loss_func = kullback_leibler_divergence
# loss_func = poisson
model.compile(optimizer=opt,
              loss= loss_func,
              metrics=['accuracy'])
fit_history = model.fit(train_data, train_label, epochs=20,
                    validation_data=(valid_data, valid_label), batch_size=batch_size)
stop_time=time.time()
print("\n training time:",stop_time-start_time)

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['accuracy'], label='training accuracy')
plt.plot(fit_history.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
#plt.ylim([0.4, 1])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['loss'], label='training loss')
plt.plot(fit_history.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 2])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
z=model.predict(test_data)
pred=np.argmax(z,axis=1)
y_label=np.argmax(test_label,axis=1)
cm = confusion_matrix(pred, y_label)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#disp.plot(cmap=plt.cm.Blues)
sns.heatmap(cm,cmap="Blues",annot=True,fmt="d")
plt.show()

# calculate loss and accuracy on test data and print it
test_loss, test_acc = model.evaluate(test_data, test_label, verbose=2)
train_loss, train_acc = model.evaluate(train_data,  train_label, verbose=2)
precision = precision_score(y_label,pred,average='weighted')
recall = recall_score(y_label,pred,average='weighted')
f1 = f1_score(y_label,pred,average='weighted')
print("accuracy on test data:",test_acc)
print("loss on test data:",test_loss)
print("f1_score on test data:",f1)
print("recall on test data:",recall)
print("presicion on test data:",precision)

"""# **PART E: Optimization Function**"""

model = models.Sequential()
activation = "relu"
model.add(layers.Dense(1024, activation = "relu", input_shape=(1024,)))
model.add(layers.Dense(1024, activation = "relu"))
model.add(layers.Dense(512, activation = "relu"))
model.add(layers.Dense(10, activation = "softmax"))
model.summary()

# training the model
batch_size = 128
start_time=time.time()
opt = SGD(learning_rate=0.01, momentum=0.9)
#opt = "RMSprop"
#opt = "adam"
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
fit_history = model.fit(train_data, train_label, epochs=20,
                    validation_data=(valid_data, valid_label), batch_size=batch_size)
stop_time=time.time()
print("\n training time:",stop_time-start_time)

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['accuracy'], label='training accuracy')
plt.plot(fit_history.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
#plt.ylim([0.4, 1])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['loss'], label='training loss')
plt.plot(fit_history.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 2])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
z=model.predict(test_data)
pred=np.argmax(z,axis=1)
y_label=np.argmax(test_label,axis=1)
cm = confusion_matrix(pred, y_label)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#disp.plot(cmap=plt.cm.Blues)
sns.heatmap(cm,cmap="Blues",annot=True,fmt="d")
plt.show()

# calculate loss and accuracy on test data and print it
test_loss, test_acc = model.evaluate(test_data, test_label, verbose=2)
train_loss, train_acc = model.evaluate(train_data,  train_label, verbose=2)
precision = precision_score(y_label,pred,average='weighted')
recall = recall_score(y_label,pred,average='weighted')
f1 = f1_score(y_label,pred,average='weighted')
print("accuracy on test data:",test_acc)
print("loss on test data:",test_loss)
print("f1_score on test data:",f1)
print("recall on test data:",recall)
print("presicion on test data:",precision)

"""# Part F: Number of Hidden Layers"""

# uncomment to see changes
model = models.Sequential()
activation = "relu"
model.add(layers.Dense(1024, activation = "relu", input_shape=(1024,)))
model.add(layers.Dense(1024, activation = "relu"))
#model.add(layers.Dense(1024, activation = "relu"))
#model.add(layers.Dense(1024, activation = "relu"))
model.add(layers.Dense(512, activation = "relu"))
model.add(layers.Dense(10, activation = "softmax"))
model.summary()

# training the model
batch_size = 128
start_time=time.time()
opt = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
fit_history = model.fit(train_data, train_label, epochs=20,
                    validation_data=(valid_data, valid_label), batch_size=batch_size)
stop_time=time.time()
print("\n training time:",stop_time-start_time)

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['accuracy'], label='training accuracy')
plt.plot(fit_history.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
#plt.ylim([0.4, 1])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
plt.plot(fit_history.history['loss'], label='training loss')
plt.plot(fit_history.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.ylim([0, 2])
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(6, 4), dpi=100)
z=model.predict(test_data)
pred=np.argmax(z,axis=1)
y_label=np.argmax(test_label,axis=1)
cm = confusion_matrix(pred, y_label)
#disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#disp.plot(cmap=plt.cm.Blues)
sns.heatmap(cm,cmap="Blues",annot=True,fmt="d")
plt.show()

# calculate loss and accuracy on test data and print it
test_loss, test_acc = model.evaluate(test_data, test_label, verbose=2)
train_loss, train_acc = model.evaluate(train_data,  train_label, verbose=2)
precision = precision_score(y_label,pred,average='weighted')
recall = recall_score(y_label,pred,average='weighted')
f1 = f1_score(y_label,pred,average='weighted')
print("accuracy on test data:",test_acc)
print("loss on test data:",test_loss)
print("f1_score on test data:",f1)
print("recall on test data:",recall)
print("presicion on test data:",precision)